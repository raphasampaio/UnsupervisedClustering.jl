var documenterSearchIndex = {"docs":
[{"location":"localsearch/kmeans/#k-means","page":"k-means","title":"k-means","text":"","category":"section"},{"location":"localsearch/kmeans/","page":"k-means","title":"k-means","text":"Modules = [UnsupervisedClustering]\nPages   = [\"localsearch/kmeans.jl\"]","category":"page"},{"location":"localsearch/kmeans/#UnsupervisedClustering.Kmeans","page":"k-means","title":"UnsupervisedClustering.Kmeans","text":"Kmeans(\n    metric::SemiMetric = SqEuclidean()\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Real = DEFAULT_TOLERANCE\n    max_iterations::Integer = DEFAULT_MAX_ITERATIONS\n)\n\nThe k-means is a clustering algorithm that aims to partition data into clusters by minimizing the distances between data points and their cluster centroids.\n\nFields\n\nmetric: defines the distance metric used to compute the distances between data points and cluster centroids.\nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the centroid positions between consecutive iterations.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\n\nReferences\n\nHartigan, John A., and Manchek A. Wong. Algorithm AS 136: A k-means clustering algorithm. Journal of the royal statistical society. series c (applied statistics) 28.1 (1979): 100-108.\nLloyd, Stuart. Least squares quantization in PCM. IEEE transactions on information theory 28.2 (1982): 129-137.\n\n\n\n\n\n","category":"type"},{"location":"localsearch/kmeans/#UnsupervisedClustering.KmeansResult","page":"k-means","title":"UnsupervisedClustering.KmeansResult","text":"KmeansResult(\n    assignments::AbstractVector{<:Integer}\n    clusters::AbstractMatrix{<:Real}\n    objective::Real\n    objective_per_cluster::AbstractVector{<:Real}\n    iterations::Integer\n    elapsed::Real\n    converged::Bool\n    k::Integer\n)\n\nKmeansResult struct represents the result of the k-means clustering algorithm.\n\nFields\n\nassignments: an integer vector that stores the cluster assignment for each data point.\nclusters: a floating-point matrix representing the cluster's centroid.\nobjective: a floating-point number representing the objective function after running the algorithm. The objective function measures the quality of the clustering solution.\nobjective_per_cluster: a floating-point vector that stores the objective function of each cluster\niterations: an integer value indicating the number of iterations performed until the algorithm has converged or reached the maximum number of iterations\nelapsed: a floating-point number representing the time in seconds for the algorithm to complete.\nconverged: indicates whether the algorithm has converged to a solution.\nk: the number of clusters.\n\n\n\n\n\n","category":"type"},{"location":"localsearch/kmeans/#UnsupervisedClustering.fit!-Tuple{Kmeans, AbstractMatrix{<:Real}, KmeansResult}","page":"k-means","title":"UnsupervisedClustering.fit!","text":"fit!(\n    kmeans::Kmeans,\n    data::AbstractMatrix{<:Real},\n    result::KmeansResult\n)\n\nThe fit! function performs the k-means clustering algorithm on the given result as the initial point and updates the provided object with the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nresult: a result object that will be updated with the clustering result.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nresult = KmeansResult(n, [1.0 2.0; 1.0 2.0])\nfit!(kmeans, data, result)\n\n\n\n\n\n","category":"method"},{"location":"localsearch/kmeans/#UnsupervisedClustering.fit-Tuple{Kmeans, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"k-means","title":"UnsupervisedClustering.fit","text":"fit(\n    kmeans::Kmeans,\n    data::AbstractMatrix{<:Real},\n    initial_clusters::AbstractVector{<:Integer}\n)\n\nThe fit function performs the k-means clustering algorithm on the given data points as the initial point and returns a result object representing the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\ninitial_clusters: an integer vector where each element is the initial data point for each cluster.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nresult = fit(kmeans, data, [4, 12])\n\n\n\n\n\n","category":"method"},{"location":"localsearch/kmeans/#UnsupervisedClustering.fit-Tuple{Kmeans, AbstractMatrix{<:Real}, Integer}","page":"k-means","title":"UnsupervisedClustering.fit","text":"fit(\n    kmeans::Kmeans,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function performs the k-means clustering algorithm and returns a result object representing the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nresult = fit(kmeans, data, k)\n\n\n\n\n\n","category":"method"},{"location":"metaheuristic/randomswap/#Random-Swap","page":"Random Swap","title":"Random Swap","text":"","category":"section"},{"location":"metaheuristic/randomswap/","page":"Random Swap","title":"Random Swap","text":"Modules = [UnsupervisedClustering]\nPages   = [\"metaheuristic/randomswap.jl\"]","category":"page"},{"location":"metaheuristic/randomswap/#UnsupervisedClustering.RandomSwap","page":"Random Swap","title":"UnsupervisedClustering.RandomSwap","text":"RandomSwap(\n    local_search::ClusteringAlgorithm\n    verbose::Bool = DEFAULT_VERBOSE\n    max_iterations::Integer = 200\n    max_iterations_without_improvement::Integer = 150\n)\n\nRandomSwap is a meta-heuristic approach used for clustering problems. It follows an iterative process that combines local optimization with perturbation to explore the search space effectively. A local optimization algorithm is applied at each iteration to converge toward a local optimum. Then, a perturbation operator generates a new starting point and continues the search.\n\nFields\n\nlocal_search: the clustering algorithm applied to improve the solution in each meta-heuristics iteration.\nverbose: controls whether the algorithm should display additional information during execution.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping.\nmax_iterations_without_improvement: represents the maximum number of iterations allowed without improving the best solution.\n\nReferences\n\nFränti, Pasi. Efficiency of random swap clustering. Journal of big data 5.1 (2018): 1-29.\n\n\n\n\n\n","category":"type"},{"location":"metaheuristic/randomswap/#UnsupervisedClustering.fit-Tuple{RandomSwap, AbstractMatrix{<:Real}, Integer}","page":"Random Swap","title":"UnsupervisedClustering.fit","text":"fit(\n    meta::RandomSwap,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function applies a random swap to a clustering problem and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nrandom_swap = RandomSwap(local_search = kmeans)\nresult = fit(random_swap, data, k)\n\n\n\n\n\n","category":"method"},{"location":"localsearch/gmm/#GMM","page":"GMM","title":"GMM","text":"","category":"section"},{"location":"localsearch/gmm/","page":"GMM","title":"GMM","text":"Modules = [UnsupervisedClustering]\nPages   = [\"localsearch/gmm.jl\"]","category":"page"},{"location":"localsearch/gmm/#UnsupervisedClustering.GMM","page":"GMM","title":"UnsupervisedClustering.GMM","text":"GMM(\n    estimator::CovarianceMatrixEstimator\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Real = DEFAULT_TOLERANCE\n    max_iterations::Integer = DEFAULT_MAX_ITERATIONS\n    decompose_if_fails::Bool = true\n)\n\nThe GMM is a clustering algorithm that models the underlying data distribution as a mixture of Gaussian distributions.\n\nFields\n\nestimator: represents the method or algorithm used to estimate the covariance matrices in the GMM. \nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the model's log-likelihood between consecutive iterations before considering convergence.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\ndecompose_if_fails: determines whether the algorithm should attempt to decompose the covariance matrix of a component and fix its eigenvalues if the decomposition fails due to numerical issues.       \n\nReferences\n\nDempster, Arthur P., Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society: series B (methodological) 39.1 (1977): 1-22.\n\n\n\n\n\n","category":"type"},{"location":"localsearch/gmm/#UnsupervisedClustering.GMMResult","page":"GMM","title":"UnsupervisedClustering.GMMResult","text":"GMMResult(\n    assignments::AbstractVector{<:Integer}\n    weights::AbstractVector{<:Real}\n    clusters::AbstractVector{<:AbstractVector{<:Real}}\n    covariances::AbstractVector{<:Symmetric{<:Real}}\n    objective::Real\n    iterations::Integer\n    elapsed::Real\n    converged::Bool\n    k::Integer\n)\n\nGMMResult struct represents the result of the GMM clustering algorithm.\n\nFields\n\nassignments: an integer vector that stores the cluster assignment for each data point.\nweights: a vector of floating-point numbers representing the weights associated with each cluster. The weight indicates the probability of a data point belonging to its respective cluster.\nclusters: a vector of floating-point vectors representing the cluster's centroid.\ncovariances: a vector of symmetric matrices, where each matrix represents the covariance matrix of a cluster in the GMM model. The covariance matrix describes the shape and orientation of the data distribution within each cluster.\nobjective: a floating-point number representing the objective function after running the algorithm. The objective function measures the quality of the clustering solution.\niterations: an integer value indicating the number of iterations performed until the algorithm has converged or reached the maximum number of iterations\nelapsed: a floating-point number representing the time in seconds for the algorithm to complete.\nconverged: indicates whether the algorithm has converged to a solution.\nk: the number of clusters.\n\n\n\n\n\n","category":"type"},{"location":"localsearch/gmm/#UnsupervisedClustering.fit!-Tuple{GMM, AbstractMatrix{<:Real}, GMMResult}","page":"GMM","title":"UnsupervisedClustering.fit!","text":"fit!(\n    gmm::GMM,\n    data::AbstractMatrix{<:Real},\n    result::GMMResult\n)\n\nThe fit! function performs the GMM clustering algorithm on the given result as the initial point and updates the provided object with the clustering result.\n\nParameters:\n\ngmm: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nresult: a result object that will be updated with the clustering result.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\nresult = GMMResult(n, [[1.0, 1.0], [2.0, 2.0]])\nfit!(gmm, data, result)\n\n\n\n\n\n","category":"method"},{"location":"localsearch/gmm/#UnsupervisedClustering.fit-Tuple{GMM, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"GMM","title":"UnsupervisedClustering.fit","text":"fit(\n    gmm::GMM,\n    data::AbstractMatrix{<:Real},\n    initial_clusters::AbstractVector{<:Integer}\n)\n\nThe fit function performs the GMM clustering algorithm on the given data points as the initial point and returns a result object representing the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\ninitial_clusters: an integer vector where each element is the initial data point for each cluster.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\nresult = fit(gmm, data, [4, 12])\n\n\n\n\n\n","category":"method"},{"location":"localsearch/gmm/#UnsupervisedClustering.fit-Tuple{GMM, AbstractMatrix{<:Real}, Integer}","page":"GMM","title":"UnsupervisedClustering.fit","text":"fit(\n    gmm::GMM,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function performs the GMM clustering algorithm and returns a result object representing the clustering result.\n\nParameters:\n\ngmm: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\nresult = fit(gmm, data, k)\n\n\n\n\n\n","category":"method"},{"location":"ensemble/chain/#Clustering-Chain","page":"Clustering Chain","title":"Clustering Chain","text":"","category":"section"},{"location":"ensemble/chain/","page":"Clustering Chain","title":"Clustering Chain","text":"Modules = [UnsupervisedClustering]\nPages   = [\"ensemble/chain.jl\"]","category":"page"},{"location":"ensemble/chain/#UnsupervisedClustering.ClusteringChain","page":"Clustering Chain","title":"UnsupervisedClustering.ClusteringChain","text":"ClusteringChain(algorithms::ClusteringAlgorithm...)\n\nClusteringChain represents a chain of clustering algorithms that are executed sequentially. It allows for applying multiple clustering algorithms in a specific order to refine and improve the clustering results.\n\nFields\n\nalgorithms: the vector of clustering algorithms that will be executed in sequence.\n\n\n\n\n\n","category":"type"},{"location":"ensemble/chain/#UnsupervisedClustering.fit-Tuple{ClusteringChain, AbstractMatrix{<:Real}, Integer}","page":"Clustering Chain","title":"UnsupervisedClustering.fit","text":"fit(chain::ClusteringChain, data::AbstractMatrix{<:Real}, k::Integer)\n\nThe fit function applies a sequence of clustering algorithms and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\n\nchain = ClusteringChain(kmeans, gmm)\nresult = fit(chain, data, k)\n\n\n\n\n\n","category":"method"},{"location":"metaheuristic/multistart/#Multi-Start","page":"Multi-Start","title":"Multi-Start","text":"","category":"section"},{"location":"metaheuristic/multistart/","page":"Multi-Start","title":"Multi-Start","text":"Modules = [UnsupervisedClustering]\nPages   = [\"metaheuristic/multistart.jl\"]","category":"page"},{"location":"metaheuristic/multistart/#UnsupervisedClustering.MultiStart","page":"Multi-Start","title":"UnsupervisedClustering.MultiStart","text":"MultiStart(\n    local_search::ClusteringAlgorithm\n    verbose::Bool = DEFAULT_VERBOSE\n    max_iterations::Integer = 200\n)\n\nThe MultiStart approach repeatedly applies a clustering algorithm to generate multiple solutions with different initial points and selects the best solution.\n\nFields\n\nlocal_search: the clustering algorithm applied to improve the solution in each meta-heuristics iteration.\nverbose: controls whether the algorithm should display additional information during execution.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping.\n\n\n\n\n\n","category":"type"},{"location":"metaheuristic/multistart/#UnsupervisedClustering.fit-Tuple{MultiStart, AbstractMatrix{<:Real}, Integer}","page":"Multi-Start","title":"UnsupervisedClustering.fit","text":"fit(\n    meta::MultiStart,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function applies a multi-start to a clustering problem and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nmulti_start = MultiStart(local_search = kmeans)\nresult = fit(multi_start, data, k)\n\n\n\n\n\n","category":"method"},{"location":"localsearch/kmedoids/#k-medoids","page":"k-medoids","title":"k-medoids","text":"","category":"section"},{"location":"localsearch/kmedoids/","page":"k-medoids","title":"k-medoids","text":"Modules = [UnsupervisedClustering]\nPages   = [\"localsearch/kmedoids.jl\"]","category":"page"},{"location":"localsearch/kmedoids/#UnsupervisedClustering.Kmedoids","page":"k-medoids","title":"UnsupervisedClustering.Kmedoids","text":"Kmedoids(\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Real = DEFAULT_TOLERANCE\n    max_iterations::Integer = DEFAULT_MAX_ITERATIONS\n)\n\nThe k-medoids is a variation of k-means clustering algorithm that uses actual data points (medoids) as representatives of each cluster instead of the mean.\n\nFields\n\nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the centroid positions between consecutive iterations.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\n\nReferences\n\n\n\n\n\n","category":"type"},{"location":"localsearch/kmedoids/#UnsupervisedClustering.KmedoidsResult","page":"k-medoids","title":"UnsupervisedClustering.KmedoidsResult","text":"KmedoidsResult(\n    assignments::AbstractVector{<:Integer}\n    clusters::AbstractVector{<:Integer}\n    objective::Real\n    objective_per_cluster::AbstractVector{<:Real}\n    iterations::Integer\n    elapsed::Real\n    converged::Bool\n    k::Integer\n)\n\nKmedoidsResult struct represents the result of the k-medoids clustering algorithm.\n\nFields\n\nassignments: an integer vector that stores the cluster assignment for each data point.\nclusters: an integer vector representing each cluster's centroid.\nobjective: a floating-point number representing the objective function after running the algorithm. The objective function measures the quality of the clustering solution.\nobjective_per_cluster: a floating-point vector that stores the objective function of each cluster\niterations: an integer value indicating the number of iterations performed until the algorithm has converged or reached the maximum number of iterations\nelapsed: a floating-point number representing the time in seconds for the algorithm to complete.\nconverged: indicates whether the algorithm has converged to a solution.\nk: the number of clusters.\n\n\n\n\n\n","category":"type"},{"location":"localsearch/kmedoids/#UnsupervisedClustering.fit!-Tuple{Kmedoids, AbstractMatrix{<:Real}, KmedoidsResult}","page":"k-medoids","title":"UnsupervisedClustering.fit!","text":"fit!(\n    kmedoids::Kmedoids,\n    distances::AbstractMatrix{<:Real},\n    result::KmedoidsResult\n)\n\nThe fit! function performs the k-medoids clustering algorithm on the given result as the initial point and updates the provided object with the clustering result.\n\nParameters:\n\nkmedoids: an instance representing the clustering settings and parameters.\ndistances: a floating-point matrix representing the pairwise distances between the data points.\nresult: a result object that will be updated with the clustering result.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\ndistances = pairwise(SqEuclidean(), data, dims = 1)\n\nkmedoids = Kmedoids()\nresult = KmedoidsResult(n, [1.0 2.0; 1.0 2.0])\nfit!(kmedoids, distances, result)\n\n\n\n\n\n","category":"method"},{"location":"localsearch/kmedoids/#UnsupervisedClustering.fit-Tuple{Kmedoids, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"k-medoids","title":"UnsupervisedClustering.fit","text":"fit(\n    kmedoids::Kmedoids,\n    distances::AbstractMatrix{<:Real},\n    initial_clusters::AbstractVector{<:Integer}\n)\n\nThe fit function performs the k-medoids clustering algorithm on the given data points as the initial point and returns a result object representing the clustering result.\n\nParameters:\n\nkmedoids: an instance representing the clustering settings and parameters.\ndistances: a floating-point matrix representing the pairwise distances between the data points.\ninitial_clusters: an integer vector where each element is the initial data point for each cluster.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\ndistances = pairwise(SqEuclidean(), data, dims = 1)\n\nkmedoids = Kmedoids()\nresult = fit(kmedoids, distances, [4, 12])\n\n\n\n\n\n","category":"method"},{"location":"localsearch/kmedoids/#UnsupervisedClustering.fit-Tuple{Kmedoids, AbstractMatrix{<:Real}, Integer}","page":"k-medoids","title":"UnsupervisedClustering.fit","text":"fit(\n    kmedoids::Kmedoids,\n    distances::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function performs the k-medoids clustering algorithm and returns a result object representing the clustering result.\n\nParameters:\n\nkmedoids: an instance representing the clustering settings and parameters.\ndistances: a floating-point matrix representing the pairwise distances between the data points.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\ndistances = pairwise(SqEuclidean(), data, dims = 1)\n\nkmedoids = Kmedoids()\nresult = fit(kmedoids, distances, k)\n\n\n\n\n\n","category":"method"},{"location":"#UnsupervisedClustering","page":"Home","title":"UnsupervisedClustering","text":"","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"UnsupervisedClustering\")","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you find UnsupervisedClustering useful in your work, we kindly request that you cite the following paper (preprint):","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{sampaio2023regularization,\n  title={Regularization and Global Optimization in Model-Based Clustering},\n  author={Sampaio, Raphael Araujo and Garcia, Joaquim Dias and Poggi, Marcus and Vidal, Thibaut},\n  journal={arXiv preprint arXiv:2302.02450},\n  year={2023}\n}","category":"page"},{"location":"metaheuristic/geneticalgorithm/#Genetic-Algorithm","page":"Genetic Algorithm","title":"Genetic Algorithm","text":"","category":"section"},{"location":"metaheuristic/geneticalgorithm/","page":"Genetic Algorithm","title":"Genetic Algorithm","text":"Modules = [UnsupervisedClustering]\nPages   = [\"metaheuristic/geneticalgorithm.jl\"]","category":"page"},{"location":"metaheuristic/geneticalgorithm/#UnsupervisedClustering.GeneticAlgorithm","page":"Genetic Algorithm","title":"UnsupervisedClustering.GeneticAlgorithm","text":"GeneticAlgorithm(\n    local_search::ClusteringAlgorithm\n    verbose::Bool = DEFAULT_VERBOSE\n    max_iterations::Integer = 200\n    max_iterations_without_improvement::Integer = 150\n    π_min::Integer = 40\n    π_max::Integer = 50\n)\n\nGeneticAlgorithm represents a clustering algorithm that utilizes a genetic algorithm approach to optimize cluster assignments. It combines evolutionary computation and local search elements to find high-quality clustering solutions.\n\nFields\n\nlocal_search: the clustering algorithm applied to improve the solution in each meta-heuristics iteration.\nverbose: controls whether the algorithm should display additional information during execution.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping.\nmax_iterations_without_improvement: represents the maximum number of iterations allowed without improving the best solution.\nπ_max: the maximum population size used in the genetic algorithm.\nπ_min: the minimum population size used in the genetic algorithm.\n\nReferences\n\n\n\n\n\n","category":"type"},{"location":"metaheuristic/geneticalgorithm/#UnsupervisedClustering.fit-Tuple{GeneticAlgorithm, AbstractMatrix{<:Real}, Integer}","page":"Genetic Algorithm","title":"UnsupervisedClustering.fit","text":"fit(\n    meta::GeneticAlgorithm,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function applies a genetic algorithm to a clustering problem and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\ngenetic_algorithm = GeneticAlgorithm(local_search = kmeans)\nresult = fit(genetic_algorithm, data, k)\n\n\n\n\n\n","category":"method"}]
}
