var documenterSearchIndex = {"docs":
[{"location":"local_search/gmm/#Gaussian-Mixture-Models-(GMM)","page":"GMM","title":"Gaussian Mixture Models (GMM)","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"Gaussian Mixture Models represent data as a mixture of Gaussian distributions, providing probabilistic cluster assignments and soft clustering capabilities. Unlike k-means, GMM can model elliptical clusters and provides uncertainty estimates.","category":"page"},{"location":"local_search/gmm/#Algorithm-Overview","page":"GMM","title":"Algorithm Overview","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"GMM uses the Expectation-Maximization (EM) algorithm:","category":"page"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"E-step: Calculate probabilities of each point belonging to each Gaussian component\nM-step: Update Gaussian parameters (means, covariances, mixing weights) based on probabilities\nRepeat: Continue until convergence of log-likelihood","category":"page"},{"location":"local_search/gmm/#Basic-Usage","page":"GMM","title":"Basic Usage","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"using UnsupervisedClustering\n\n# Generate sample data\ndata = rand(100, 2)\nk = 3\n\n# Create covariance estimator (required for GMM)\nn, d = size(data)\nestimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\n# Create and run GMM\ngmm = GMM(estimator = estimator)\nresult = fit(gmm, data, k)\n\nprintln(\"Log-likelihood: $(result.objective)\")\nprintln(\"Converged: $(result.converged)\")","category":"page"},{"location":"local_search/gmm/#Covariance-Matrix-Estimation","page":"GMM","title":"Covariance Matrix Estimation","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"GMM requires a covariance estimator to handle numerical stability:","category":"page"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"# Different estimator types\nn, d = size(data)\n\n# Empirical covariance (default)\nemp_estimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\n# For integration with RegularizedCovarianceMatrices.jl\n# regularized_estimator = SomeRegularizedEstimator(n, d)\n\ngmm_empirical = GMM(estimator = emp_estimator)\nresult = fit(gmm_empirical, data, k)","category":"page"},{"location":"local_search/gmm/#Comparison-with-K-means","page":"GMM","title":"Comparison with K-means","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"using UnsupervisedClustering, Random\n\n# Create elliptical clusters (GMM should perform better)\nRandom.seed!(42)\ncluster1 = [randn(50) randn(50) * 0.3] .+ [2, 2]   # Elongated cluster\ncluster2 = [randn(50) * 0.3 randn(50)] .+ [-2, -2]  # Elongated cluster\ncluster3 = randn(50, 2) .+ [0, 3]                   # Circular cluster\ndata = vcat(cluster1, cluster2, cluster3)\n\nn, d = size(data)\nestimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\n# Compare algorithms\nalgorithms = [\n    (\"K-means\", Kmeans()),\n    (\"GMM\", GMM(estimator = estimator))\n]\n\nprintln(\"Algorithm Comparison on Elliptical Data:\")\nfor (name, alg) in algorithms\n    result = fit(alg, data, 3)\n    println(\"$name: objective = $(round(result.objective, digits=3))\")\nend","category":"page"},{"location":"local_search/gmm/#Configuration-Options","page":"GMM","title":"Configuration Options","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"# Customize GMM parameters\nn, d = size(data)\nestimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\ngmm = GMM(\n    estimator = estimator,\n    tolerance = 1e-6,           # Convergence threshold\n    max_iterations = 1000,      # Maximum EM iterations\n    verbose = false             # Print progress\n)\n\nresult = fit(gmm, data, k)","category":"page"},{"location":"local_search/gmm/#Probabilistic-Clustering","page":"GMM","title":"Probabilistic Clustering","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"GMM provides soft cluster assignments (probabilities):","category":"page"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"# Get clustering result\nresult = fit(GMM(estimator = estimator), data, 3)\n\n# Hard assignments (like k-means)\nhard_assignments = result.assignments\n\n# For soft assignments, you would typically access the posterior probabilities\n# This requires running the E-step separately or modifying the algorithm\nprintln(\"Hard cluster assignments: $(hard_assignments[1:10])\")","category":"page"},{"location":"local_search/gmm/#Advanced-Example-with-Model-Selection","page":"GMM","title":"Advanced Example with Model Selection","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"using UnsupervisedClustering, Random\n\n# Create complex dataset\nRandom.seed!(42)\ncluster1 = randn(40, 3) .+ [3, 3, 3]\ncluster2 = randn(40, 3) .+ [-3, -3, -3]\ncluster3 = randn(40, 3) .+ [3, -3, 0]\ndata = vcat(cluster1, cluster2, cluster3)\n\n# Try different numbers of components\nk_values = 2:6\nlog_likelihoods = Float64[]\n\nn, d = size(data)\nestimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\nprintln(\"Model Selection for GMM:\")\nfor k in k_values\n    gmm = GMM(estimator = estimator, max_iterations = 200)\n    result = fit(gmm, data, k)\n    push!(log_likelihoods, result.objective)\n    println(\"k=$k: log-likelihood = $(round(result.objective, digits=3))\")\nend\n\n# Higher log-likelihood indicates better fit\nbest_k = k_values[argmax(log_likelihoods)]\nprintln(\"Best k based on log-likelihood: $best_k\")","category":"page"},{"location":"local_search/gmm/#Integration-with-Other-Algorithms","page":"GMM","title":"Integration with Other Algorithms","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"# Use GMM in algorithm chains\nn, d = size(data)\nestimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\n# Initialize with k-means, refine with GMM\nchain = ClusteringChain(\n    Kmeans(max_iterations = 50),\n    GMM(estimator = estimator, max_iterations = 100)\n)\n\nresult_chain = fit(chain, data, 3)\nprintln(\"Chained k-means → GMM: $(result_chain.objective)\")\n\n# Use GMM with metaheuristics\ngenetic_gmm = GeneticAlgorithm(\n    local_search = GMM(estimator = estimator, max_iterations = 50),\n    max_iterations = 50\n)\n\nresult_genetic = fit(genetic_gmm, data, 3)\nprintln(\"Genetic GMM: $(result_genetic.objective)\")","category":"page"},{"location":"local_search/gmm/#Handling-High-Dimensional-Data","page":"GMM","title":"Handling High-Dimensional Data","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"# For high-dimensional data, consider regularization\nhigh_dim_data = rand(100, 20)  # 20-dimensional data\nn, d = size(high_dim_data)\n\n# Use empirical estimator\nestimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\n# Reduce iterations for efficiency\ngmm_hd = GMM(\n    estimator = estimator,\n    max_iterations = 100,\n    tolerance = 1e-4\n)\n\nresult = fit(gmm_hd, high_dim_data, 5)\nprintln(\"High-dimensional GMM: $(result.objective)\")","category":"page"},{"location":"local_search/gmm/#Performance-Comparison","page":"GMM","title":"Performance Comparison","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"using Statistics\n\n# Compare convergence behavior\nobjectives_gmm = Float64[]\nobjectives_kmeans = Float64[]\n\nfor trial in 1:10\n    # K-means\n    result_km = fit(Kmeans(), data, 3)\n    push!(objectives_kmeans, result_km.objective)\n\n    # GMM\n    n, d = size(data)\n    estimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n    result_gmm = fit(GMM(estimator = estimator), data, 3)\n    push!(objectives_gmm, result_gmm.objective)\nend\n\nprintln(\"K-means objective statistics:\")\nprintln(\"  Mean: $(round(mean(objectives_kmeans), digits=3))\")\nprintln(\"  Std:  $(round(std(objectives_kmeans), digits=3))\")\n\nprintln(\"GMM log-likelihood statistics:\")\nprintln(\"  Mean: $(round(mean(objectives_gmm), digits=3))\")\nprintln(\"  Std:  $(round(std(objectives_gmm), digits=3))\")","category":"page"},{"location":"local_search/gmm/#When-to-Use-GMM","page":"GMM","title":"When to Use GMM","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"GMM is preferred when:","category":"page"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"Probabilistic clusters: Need soft assignments or uncertainty estimates\nNon-spherical clusters: Data has elliptical or complex cluster shapes\nOverlapping clusters: Clusters have significant overlap\nModel-based approach: Want principled statistical foundation\nDensity estimation: Need to model data distribution, not just clustering","category":"page"},{"location":"local_search/gmm/#Limitations","page":"GMM","title":"Limitations","text":"","category":"section"},{"location":"local_search/gmm/","page":"GMM","title":"GMM","text":"Computational cost: More expensive than k-means\nCovariance estimation: Requires careful handling in high dimensions\nLocal optima: Can get stuck like other EM-based methods\nModel assumptions: Assumes Gaussian distributions","category":"page"},{"location":"local_search/gmm/#API-Reference","page":"GMM","title":"API Reference","text":"","category":"section"},{"location":"local_search/gmm/#UnsupervisedClustering.GMM","page":"GMM","title":"UnsupervisedClustering.GMM","text":"GMM(\n    estimator::CovarianceMatrixEstimator\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Float64 = DEFAULT_TOLERANCE\n    max_iterations::Int = DEFAULT_MAX_ITERATIONS\n    decompose_if_fails::Bool = true\n)\n\nThe GMM is a clustering algorithm that models the underlying data distribution as a mixture of Gaussian distributions.\n\nFields\n\nestimator: represents the method or algorithm used to estimate the covariance matrices in the GMM. \nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the model's log-likelihood between consecutive iterations before considering convergence.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\ndecompose_if_fails: determines whether the algorithm should attempt to decompose the covariance matrix of a component and fix its eigenvalues if the decomposition fails due to numerical issues.       \n\nReferences\n\nDempster, Arthur P., Nan M. Laird, and Donald B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society: series B (methodological) 39.1 (1977): 1-22.\n\n\n\n\n\n","category":"type"},{"location":"local_search/gmm/#UnsupervisedClustering.GMMResult","page":"GMM","title":"UnsupervisedClustering.GMMResult","text":"GMMResult(\n    assignments::AbstractVector{<:Integer}\n    weights::AbstractVector{<:Real}\n    clusters::AbstractVector{<:AbstractVector{<:Real}}\n    covariances::AbstractVector{<:Symmetric{<:Real}}\n    objective::Real\n    iterations::Integer\n    elapsed::Real\n    converged::Bool\n    k::Integer\n)\n\nGMMResult struct represents the result of the GMM clustering algorithm.\n\nFields\n\nassignments: an integer vector that stores the cluster assignment for each data point.\nweights: a vector of floating-point numbers representing the weights associated with each cluster. The weight indicates the probability of a data point belonging to its respective cluster.\nclusters: a vector of floating-point vectors representing the cluster's centroid.\ncovariances: a vector of symmetric matrices, where each matrix represents the covariance matrix of a cluster in the GMM model. The covariance matrix describes the shape and orientation of the data distribution within each cluster.\nobjective: a floating-point number representing the objective function after running the algorithm. The objective function measures the quality of the clustering solution.\niterations: an integer value indicating the number of iterations performed until the algorithm has converged or reached the maximum number of iterations\nelapsed: a floating-point number representing the time in seconds for the algorithm to complete.\nconverged: indicates whether the algorithm has converged to a solution.\nk: the number of clusters.\n\n\n\n\n\n","category":"type"},{"location":"local_search/gmm/#UnsupervisedClustering.fit!-Tuple{GMM, AbstractMatrix{<:Real}, UnsupervisedClustering.GMMResult}","page":"GMM","title":"UnsupervisedClustering.fit!","text":"fit!(\n    gmm::GMM,\n    data::AbstractMatrix{<:Real},\n    result::GMMResult\n)\n\nThe fit! function performs the GMM clustering algorithm on the given result as the initial point and updates the provided object with the clustering result.\n\nParameters:\n\ngmm: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nresult: a result object that will be updated with the clustering result.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\nresult = GMMResult(n, [[1.0, 1.0], [2.0, 2.0]])\nfit!(gmm, data, result)\n\n\n\n\n\n","category":"method"},{"location":"local_search/gmm/#UnsupervisedClustering.fit-Tuple{GMM, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"GMM","title":"UnsupervisedClustering.fit","text":"fit(\n    gmm::GMM,\n    data::AbstractMatrix{<:Real},\n    initial_clusters::AbstractVector{<:Integer}\n)\n\nThe fit function performs the GMM clustering algorithm on the given data points as the initial point and returns a result object representing the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\ninitial_clusters: an integer vector where each element is the initial data point for each cluster.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\nresult = fit(gmm, data, [4, 12])\n\n\n\n\n\n","category":"method"},{"location":"local_search/gmm/#UnsupervisedClustering.fit-Tuple{GMM, AbstractMatrix{<:Real}, Integer}","page":"GMM","title":"UnsupervisedClustering.fit","text":"fit(\n    gmm::GMM,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function performs the GMM clustering algorithm and returns a result object representing the clustering result.\n\nParameters:\n\ngmm: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\nresult = fit(gmm, data, k)\n\n\n\n\n\n","category":"method"},{"location":"local_search/kmeans/#K-means-Clustering","page":"k-means","title":"K-means Clustering","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"K-means is one of the most popular clustering algorithms. It partitions data into k clusters by minimizing the within-cluster sum of squared distances to cluster centroids.","category":"page"},{"location":"local_search/kmeans/#Algorithm-Overview","page":"k-means","title":"Algorithm Overview","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"The k-means algorithm iteratively:","category":"page"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"Assigns each point to the nearest cluster centroid\nUpdates centroids to the mean of assigned points\nRepeats until convergence or maximum iterations","category":"page"},{"location":"local_search/kmeans/#Basic-Usage","page":"k-means","title":"Basic Usage","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"using UnsupervisedClustering\n\n# Generate sample data\ndata = rand(100, 2)\nk = 3\n\n# Create and run k-means\nkmeans = Kmeans()\nresult = fit(kmeans, data, k)\n\nprintln(\"Objective: $(result.objective)\")\nprintln(\"Converged: $(result.converged)\")","category":"page"},{"location":"local_search/kmeans/#Variants","page":"k-means","title":"Variants","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"UnsupervisedClustering.jl provides two k-means variants:","category":"page"},{"location":"local_search/kmeans/#Standard-K-means","page":"k-means","title":"Standard K-means","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"kmeans = Kmeans(\n    metric = SqEuclidean(),      # Distance metric\n    tolerance = 1e-3,            # Convergence threshold\n    max_iterations = 1000,       # Maximum iterations\n    verbose = false              # Print progress\n)","category":"page"},{"location":"local_search/kmeans/#Balanced-K-means","page":"k-means","title":"Balanced K-means","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"Forces approximately equal cluster sizes:","category":"page"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"balanced_kmeans = BalancedKmeans(\n    metric = SqEuclidean(),\n    tolerance = 1e-3,\n    max_iterations = 1000\n)\n\nresult = fit(balanced_kmeans, data, k)","category":"page"},{"location":"local_search/kmeans/#Advanced-Example","page":"k-means","title":"Advanced Example","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"using UnsupervisedClustering, Random, Statistics\n\n# Create structured data with known clusters\nRandom.seed!(42)\ncluster1 = randn(50, 2) .+ [2, 2]\ncluster2 = randn(50, 2) .+ [-2, -2]\ncluster3 = randn(50, 2) .+ [2, -2]\ndata = vcat(cluster1, cluster2, cluster3)\n\n# Standardize data (recommended)\ndata_std = (data .- mean(data, dims=1)) ./ std(data, dims=1)\n\n# Configure k-means with custom parameters\nkmeans = Kmeans(\n    tolerance = 1e-6,     # High precision\n    max_iterations = 500,\n    verbose = true        # Show progress\n)\n\nresult = fit(kmeans, data_std, 3)\n\n# Analyze results\nprintln(\"Clusters found: $(result.k)\")\nprintln(\"Final objective: $(result.objective)\")\nprintln(\"Iterations needed: $(result.iterations)\")\n\n# Check cluster assignments\nfor i in 1:3\n    cluster_points = sum(result.assignments .== i)\n    println(\"Cluster $i: $cluster_points points\")\nend","category":"page"},{"location":"local_search/kmeans/#Parameter-Selection","page":"k-means","title":"Parameter Selection","text":"","category":"section"},{"location":"local_search/kmeans/#Choosing-the-Number-of-Clusters-(k)","page":"k-means","title":"Choosing the Number of Clusters (k)","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"Use the elbow method to find optimal k:","category":"page"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"function elbow_method(data, max_k=10)\n    objectives = Float64[]\n\n    for k in 1:max_k\n        if k > size(data, 1)\n            break\n        end\n\n        result = fit(Kmeans(), data, k)\n        push!(objectives, result.objective)\n    end\n\n    return objectives\nend\n\n# Find elbow point\nobjectives = elbow_method(data_std, 8)\n\nprintln(\"Objectives by k:\")\nfor (k, obj) in enumerate(objectives)\n    println(\"k=$k: $(round(obj, digits=3))\")\nend","category":"page"},{"location":"local_search/kmeans/#Distance-Metrics","page":"k-means","title":"Distance Metrics","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"K-means supports different distance metrics:","category":"page"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"using Distances\n\n# Euclidean distance (default)\nkmeans_euclidean = Kmeans(metric = Euclidean())\n\n# Squared Euclidean (faster, same results)\nkmeans_sq_euclidean = Kmeans(metric = SqEuclidean())\n\n# Manhattan distance\nkmeans_manhattan = Kmeans(metric = Cityblock())\n\n# Compare results\nalgorithms = [\n    (\"Euclidean\", kmeans_euclidean),\n    (\"Squared Euclidean\", kmeans_sq_euclidean),\n    (\"Manhattan\", kmeans_manhattan)\n]\n\nfor (name, alg) in algorithms\n    result = fit(alg, data, 3)\n    println(\"$name: objective = $(round(result.objective, digits=3))\")\nend","category":"page"},{"location":"local_search/kmeans/#Integration-with-Metaheuristics","page":"k-means","title":"Integration with Metaheuristics","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"K-means works as a local search component in metaheuristic algorithms:","category":"page"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"# Use k-means in genetic algorithm\ngenetic_kmeans = GeneticAlgorithm(\n    local_search = Kmeans(max_iterations = 50),\n    max_iterations = 100\n)\n\n# Use k-means in multi-start\nmulti_start_kmeans = MultiStart(\n    local_search = Kmeans(),\n    max_iterations = 20\n)\n\n# Compare with standard k-means\nstandard_result = fit(Kmeans(), data, 3)\ngenetic_result = fit(genetic_kmeans, data, 3)\nmulti_result = fit(multi_start_kmeans, data, 3)\n\nprintln(\"Standard k-means: $(standard_result.objective)\")\nprintln(\"Genetic k-means: $(genetic_result.objective)\")\nprintln(\"Multi-start k-means: $(multi_result.objective)\")","category":"page"},{"location":"local_search/kmeans/#Performance-Considerations","page":"k-means","title":"Performance Considerations","text":"","category":"section"},{"location":"local_search/kmeans/#Large-Datasets","page":"k-means","title":"Large Datasets","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"# For large datasets, reduce iterations\nfast_kmeans = Kmeans(\n    max_iterations = 50,\n    tolerance = 1e-2\n)\n\n# Or use with multi-start for quality\nfast_multi = MultiStart(\n    local_search = fast_kmeans,\n    max_iterations = 5\n)","category":"page"},{"location":"local_search/kmeans/#Reproducibility","page":"k-means","title":"Reproducibility","text":"","category":"section"},{"location":"local_search/kmeans/","page":"k-means","title":"k-means","text":"using StableRNGs\n\n# Deterministic results across platforms\nrng = StableRNG(42)\nkmeans_reproducible = Kmeans(rng = rng)\nresult = fit(kmeans_reproducible, data, 3)","category":"page"},{"location":"local_search/kmeans/#API-Reference","page":"k-means","title":"API Reference","text":"","category":"section"},{"location":"local_search/kmeans/#UnsupervisedClustering.BalancedKmeans","page":"k-means","title":"UnsupervisedClustering.BalancedKmeans","text":"BalancedKmeans(\n    metric::SemiMetric = SqEuclidean()\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Float64 = DEFAULT_TOLERANCE\n    max_iterations::Int = DEFAULT_MAX_ITERATIONS\n)\n\nThe balanced kmeans is a variation of the k-means clustering algorithm that balances the number of data points assigned to each cluster.\n\nFields\n\nmetric: defines the distance metric used to compute the distances between data points and cluster centroids.\nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the centroid positions between consecutive iterations.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\n\n\n\n\n\n","category":"type"},{"location":"local_search/kmeans/#UnsupervisedClustering.Kmeans","page":"k-means","title":"UnsupervisedClustering.Kmeans","text":"Kmeans(\n    metric::SemiMetric = SqEuclidean()\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Float64 = DEFAULT_TOLERANCE\n    max_iterations::Int = DEFAULT_MAX_ITERATIONS\n)\n\nThe k-means is a clustering algorithm that aims to partition data into clusters by minimizing the distances between data points and their cluster centroids.\n\nFields\n\nmetric: defines the distance metric used to compute the distances between data points and cluster centroids.\nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the centroid positions between consecutive iterations.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\n\nReferences\n\nHartigan, John A., and Manchek A. Wong. Algorithm AS 136: A k-means clustering algorithm. Journal of the royal statistical society. series c (applied statistics) 28.1 (1979): 100-108.\nLloyd, Stuart. Least squares quantization in PCM. IEEE transactions on information theory 28.2 (1982): 129-137.\n\n\n\n\n\n","category":"type"},{"location":"local_search/kmeans/#UnsupervisedClustering.KmeansResult","page":"k-means","title":"UnsupervisedClustering.KmeansResult","text":"KmeansResult(\n    assignments::AbstractVector{<:Integer}\n    clusters::AbstractMatrix{<:Real}\n    objective::Real\n    objective_per_cluster::AbstractVector{<:Real}\n    iterations::Integer\n    elapsed::Real\n    converged::Bool\n    k::Integer\n)\n\nKmeansResult struct represents the result of the k-means clustering algorithm.\n\nFields\n\nassignments: an integer vector that stores the cluster assignment for each data point.\nclusters: a floating-point matrix representing the cluster's centroid.\nobjective: a floating-point number representing the objective function after running the algorithm. The objective function measures the quality of the clustering solution.\nobjective_per_cluster: a floating-point vector that stores the objective function of each cluster\niterations: an integer value indicating the number of iterations performed until the algorithm has converged or reached the maximum number of iterations\nelapsed: a floating-point number representing the time in seconds for the algorithm to complete.\nconverged: indicates whether the algorithm has converged to a solution.\nk: the number of clusters.\n\n\n\n\n\n","category":"type"},{"location":"local_search/kmeans/#UnsupervisedClustering.fit!-Tuple{UnsupervisedClustering.AbstractKmeans, AbstractMatrix{<:Real}, UnsupervisedClustering.KmeansResult}","page":"k-means","title":"UnsupervisedClustering.fit!","text":"fit!(\n    kmeans::AbstractKmeans,\n    data::AbstractMatrix{<:Real},\n    result::KmeansResult\n)\n\nThe fit! function performs the k-means clustering algorithm on the given result as the initial point and updates the provided object with the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nresult: a result object that will be updated with the clustering result.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nresult = KmeansResult(n, [1.0 2.0; 1.0 2.0])\nfit!(kmeans, data, result)\n\n\n\n\n\n","category":"method"},{"location":"local_search/kmeans/#UnsupervisedClustering.fit-Tuple{UnsupervisedClustering.AbstractKmeans, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"k-means","title":"UnsupervisedClustering.fit","text":"fit(\n    kmeans::AbstractKmeans,\n    data::AbstractMatrix{<:Real},\n    initial_clusters::AbstractVector{<:Integer}\n)\n\nThe fit function performs the k-means clustering algorithm on the given data points as the initial point and returns a result object representing the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\ninitial_clusters: an integer vector where each element is the initial data point for each cluster.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nresult = fit(kmeans, data, [4, 12])\n\n\n\n\n\n","category":"method"},{"location":"local_search/kmeans/#UnsupervisedClustering.fit-Tuple{UnsupervisedClustering.AbstractKmeans, AbstractMatrix{<:Real}, Integer}","page":"k-means","title":"UnsupervisedClustering.fit","text":"fit(\n    kmeans::AbstractKmeans,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function performs the k-means clustering algorithm and returns a result object representing the clustering result.\n\nParameters:\n\nkmeans: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nresult = fit(kmeans, data, k)\n\n\n\n\n\n","category":"method"},{"location":"metaheuristic/random_swap/#Random-Swap","page":"Random Swap","title":"Random Swap","text":"","category":"section"},{"location":"metaheuristic/random_swap/#UnsupervisedClustering.RandomSwap","page":"Random Swap","title":"UnsupervisedClustering.RandomSwap","text":"RandomSwap{LS}(\n    local_search::LS\n    verbose::Bool = DEFAULT_VERBOSE\n    max_iterations::Int = 200\n    max_iterations_without_improvement::Int = 150\n) where {LS <: AbstractAlgorithm}\n\nRandomSwap is a meta-heuristic approach used for clustering problems. It follows an iterative process that combines local optimization with perturbation to explore the search space effectively. A local optimization algorithm is applied at each iteration to converge toward a local optimum. Then, a perturbation operator generates a new starting point and continues the search.\n\nType Parameters\n\nLS: the specific type of the local search algorithm\n\nFields\n\nlocal_search: the clustering algorithm applied to improve the solution in each meta-heuristics iteration.\nverbose: controls whether the algorithm should display additional information during execution.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping.\nmax_iterations_without_improvement: represents the maximum number of iterations allowed without improving the best solution.\n\nReferences\n\nFränti, Pasi. Efficiency of random swap clustering. Journal of big data 5.1 (2018): 1-29.\n\n\n\n\n\n","category":"type"},{"location":"metaheuristic/random_swap/#UnsupervisedClustering.fit-Union{Tuple{LS}, Tuple{RandomSwap{LS}, AbstractMatrix{<:Real}, Integer}} where LS<:UnsupervisedClustering.AbstractAlgorithm","page":"Random Swap","title":"UnsupervisedClustering.fit","text":"fit(\n    meta::RandomSwap,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function applies a random swap to a clustering problem and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nrandom_swap = RandomSwap(local_search = kmeans)\nresult = fit(random_swap, data, k)\n\n\n\n\n\n","category":"method"},{"location":"local_search/kmeanspp/#K-means-Clustering","page":"k-means++","title":"K-means++ Clustering","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"K-means++ is an improved version of k-means that uses a smarter initialization strategy. It selects initial centroids with probability proportional to their squared distance from existing centroids, leading to better clustering quality and faster convergence.","category":"page"},{"location":"local_search/kmeanspp/#Algorithm-Overview","page":"k-means++","title":"Algorithm Overview","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"K-means++ uses the same iterative process as standard k-means but with enhanced initialization:","category":"page"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"Smart Initialization: Select centroids using distance-proportional probability\nAssignment: Assign points to nearest centroids\nUpdate: Recalculate centroids as cluster means\nRepeat: Continue until convergence","category":"page"},{"location":"local_search/kmeanspp/#Basic-Usage","page":"k-means++","title":"Basic Usage","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"using UnsupervisedClustering\n\n# Generate sample data\ndata = rand(100, 2)\nk = 3\n\n# Create and run K-means++\nkmeanspp = KmeansPlusPlus()\nresult = fit(kmeanspp, data, k)\n\nprintln(\"Objective: $(result.objective)\")\nprintln(\"Converged: $(result.converged)\")","category":"page"},{"location":"local_search/kmeanspp/#Comparison-with-Standard-K-means","page":"k-means++","title":"Comparison with Standard K-means","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"using UnsupervisedClustering, Random\n\n# Create structured data with clear clusters\nRandom.seed!(42)\ncluster1 = randn(50, 2) .+ [3, 3]\ncluster2 = randn(50, 2) .+ [-3, -3]\ncluster3 = randn(50, 2) .+ [3, -3]\ndata = vcat(cluster1, cluster2, cluster3)\n\n# Compare algorithms\nalgorithms = [\n    (\"Standard K-means\", Kmeans()),\n    (\"K-means++\", KmeansPlusPlus())\n]\n\nprintln(\"Algorithm Comparison:\")\nfor (name, alg) in algorithms\n    result = fit(alg, data, 3)\n    println(\"$name: objective = $(round(result.objective, digits=3))\")\nend","category":"page"},{"location":"local_search/kmeanspp/#Configuration-Options","page":"k-means++","title":"Configuration Options","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"# Customize K-means++ parameters\nkmeanspp = KmeansPlusPlus(\n    metric = SqEuclidean(),      # Distance metric\n    tolerance = 1e-6,            # Convergence threshold\n    max_iterations = 1000,       # Maximum iterations\n    verbose = false              # Print progress\n)\n\nresult = fit(kmeanspp, data, k)","category":"page"},{"location":"local_search/kmeanspp/#Multiple-Runs-for-Robustness","page":"k-means++","title":"Multiple Runs for Robustness","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"K-means++ provides better initialization, but multiple runs can still improve results:","category":"page"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"# Run multiple times and select best result\nbest_objective = Inf\nbest_result = nothing\n\nfor i in 1:10\n    result = fit(KmeansPlusPlus(), data, 3)\n    if result.objective < best_objective\n        best_objective = result.objective\n        best_result = result\n    end\nend\n\nprintln(\"Best objective after 10 runs: $(best_objective)\")","category":"page"},{"location":"local_search/kmeanspp/#Integration-with-Metaheuristics","page":"k-means++","title":"Integration with Metaheuristics","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"Use K-means++ as a local search component:","category":"page"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"# Genetic algorithm with K-means++ local search\ngenetic_kmeanspp = GeneticAlgorithm(\n    local_search = KmeansPlusPlus(max_iterations = 50),\n    max_iterations = 100\n)\n\n# Multi-start with K-means++\nmulti_kmeanspp = MultiStart(\n    local_search = KmeansPlusPlus(),\n    max_iterations = 20\n)\n\n# Compare results\nstandard_result = fit(KmeansPlusPlus(), data, 3)\ngenetic_result = fit(genetic_kmeanspp, data, 3)\nmulti_result = fit(multi_kmeanspp, data, 3)\n\nprintln(\"Standard K-means++: $(standard_result.objective)\")\nprintln(\"Genetic K-means++: $(genetic_result.objective)\")\nprintln(\"Multi-start K-means++: $(multi_result.objective)\")","category":"page"},{"location":"local_search/kmeanspp/#Performance-Benefits","page":"k-means++","title":"Performance Benefits","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"using Statistics\n\n# Measure consistency across multiple runs\nobjectives_kmeans = Float64[]\nobjectives_kmeanspp = Float64[]\n\nfor i in 1:20\n    result1 = fit(Kmeans(), data, 3)\n    result2 = fit(KmeansPlusPlus(), data, 3)\n\n    push!(objectives_kmeans, result1.objective)\n    push!(objectives_kmeanspp, result2.objective)\nend\n\nprintln(\"Standard K-means:\")\nprintln(\"  Mean: $(round(mean(objectives_kmeans), digits=3))\")\nprintln(\"  Std:  $(round(std(objectives_kmeans), digits=3))\")\n\nprintln(\"K-means++:\")\nprintln(\"  Mean: $(round(mean(objectives_kmeanspp), digits=3))\")\nprintln(\"  Std:  $(round(std(objectives_kmeanspp), digits=3))\")","category":"page"},{"location":"local_search/kmeanspp/#When-to-Use-K-means","page":"k-means++","title":"When to Use K-means++","text":"","category":"section"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"K-means++ is recommended when:","category":"page"},{"location":"local_search/kmeanspp/","page":"k-means++","title":"k-means++","text":"You need consistent, high-quality results\nStandard k-means shows high variance across runs\nInitialization quality significantly impacts your application\nYou want faster convergence than standard k-means","category":"page"},{"location":"local_search/kmeanspp/#API-Reference","page":"k-means++","title":"API Reference","text":"","category":"section"},{"location":"local_search/kmeanspp/#UnsupervisedClustering.KmeansPlusPlus","page":"k-means++","title":"UnsupervisedClustering.KmeansPlusPlus","text":"KmeansPlusPlus{RNG}(\n    metric::SemiMetric = SqEuclidean()\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::RNG = Random.GLOBAL_RNG\n    tolerance::Float64 = DEFAULT_TOLERANCE\n    max_iterations::Int = DEFAULT_MAX_ITERATIONS\n) where {RNG <: AbstractRNG}\n\nK-means++ is an improvement over the standard K-means algorithm that provides better initialization by selecting initial centroids with probability proportional to their squared distance from existing centroids. This typically leads to better clustering results and faster convergence.\n\nType Parameters\n\nRNG: the specific type of the random number generator\n\nFields\n\nmetric: defines the distance metric used to compute the distances between data points and cluster centroids.\nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the centroid positions between consecutive iterations.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\n\nReferences\n\nArthur, David, and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. 2007.\n\n\n\n\n\n","category":"type"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This guide will walk you through the basics of using UnsupervisedClustering.jl, from installation to advanced usage patterns.","category":"page"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"UnsupervisedClustering\")","category":"page"},{"location":"getting_started/#Basic-Usage","page":"Getting Started","title":"Basic Usage","text":"","category":"section"},{"location":"getting_started/#Your-First-Clustering","page":"Getting Started","title":"Your First Clustering","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using UnsupervisedClustering\nusing Random\n\n# Set seed for reproducibility\nRandom.seed!(42)\n\n# Generate sample data: 100 points in 2D space\ndata = rand(100, 2)\n\n# Create a k-means algorithm instance\nkmeans = Kmeans()\n\n# Perform clustering with 3 clusters\nresult = fit(kmeans, data, 3)\n\nprintln(\"Clustering completed!\")\nprintln(\"Number of clusters: $(result.k)\")\nprintln(\"Objective value: $(result.objective)\")\nprintln(\"Converged: $(result.converged)\")\nprintln(\"Iterations: $(result.iterations)\")","category":"page"},{"location":"getting_started/#Understanding-Results","page":"Getting Started","title":"Understanding Results","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"All algorithms return result objects with a consistent interface:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Access cluster assignments\nassignments = result.assignments  # Vector indicating cluster for each point\nprintln(\"First 10 assignments: $(assignments[1:10])\")\n\n# Access clustering objective (lower is usually better)\nobjective = result.objective\n\n# Check convergence status\nif result.converged\n    println(\"Algorithm converged in $(result.iterations) iterations\")\nelse\n    println(\"Algorithm did not converge\")\nend\n\n# Execution time\nprintln(\"Clustering took $(result.elapsed) seconds\")","category":"page"},{"location":"getting_started/#Trying-Different-Algorithms","page":"Getting Started","title":"Trying Different Algorithms","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The beauty of UnsupervisedClustering.jl is the unified interface. Let's compare multiple algorithms:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using UnsupervisedClustering\nRandom.seed!(42)\n\n# Create a more interesting dataset\nn, d, k = 150, 2, 3\ndata = vcat(\n    randn(50, 2) .+ [2, 2],   # Cluster 1\n    randn(50, 2) .+ [-2, 2],  # Cluster 2\n    randn(50, 2) .+ [0, -2]   # Cluster 3\n)\n\n# Try different algorithms\nalgorithms = [\n    (\"K-means\", Kmeans()),\n    (\"K-means++\", KmeansPlusPlus()),\n    (\"K-medoids\", Kmedoids()),\n]\n\nprintln(\"Algorithm Comparison:\")\nprintln(\"=\" ^ 50)\n\nfor (name, algorithm) in algorithms\n    result = fit(algorithm, data, k)\n    println(\"$name:\")\n    println(\"  Objective: $(round(result.objective, digits=3))\")\n    println(\"  Iterations: $(result.iterations)\")\n    println(\"  Time: $(round(result.elapsed, digits=4))s\")\n    println()\nend","category":"page"},{"location":"getting_started/#Advanced-Algorithms","page":"Getting Started","title":"Advanced Algorithms","text":"","category":"section"},{"location":"getting_started/#Metaheuristic-Approaches","page":"Getting Started","title":"Metaheuristic Approaches","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"For challenging datasets where local optima are a concern:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Genetic Algorithm for global optimization\ngenetic = GeneticAlgorithm(\n    local_search = Kmeans(),\n    max_iterations = 100,\n    π_max = 50,  # Population size\n    verbose = false\n)\n\nresult_genetic = fit(genetic, data, k)\nprintln(\"Genetic Algorithm objective: $(result_genetic.objective)\")\n\n# Multi-Start for robust results\nmulti_start = MultiStart(\n    local_search = KmeansPlusPlus(),\n    max_iterations = 20\n)\n\nresult_multi = fit(multi_start, data, k)\nprintln(\"Multi-Start objective: $(result_multi.objective)\")","category":"page"},{"location":"getting_started/#Gaussian-Mixture-Models","page":"Getting Started","title":"Gaussian Mixture Models","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"For probabilistic clustering:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# GMM requires an estimator\nn, d = size(data)\nestimator = UnsupervisedClustering.EmpiricalCovarianceMatrix(n, d)\n\ngmm = GMM(\n    estimator = estimator,\n    max_iterations = 100,\n    tolerance = 1e-6\n)\n\nresult_gmm = fit(gmm, data, k)\nprintln(\"GMM log-likelihood: $(result_gmm.objective)\")","category":"page"},{"location":"getting_started/#Algorithm-Composition","page":"Getting Started","title":"Algorithm Composition","text":"","category":"section"},{"location":"getting_started/#Chaining-Algorithms","page":"Getting Started","title":"Chaining Algorithms","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Combine algorithms for improved results:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# First apply k-means, then refine with GMM\nchain = ClusteringChain(\n    Kmeans(max_iterations = 10),\n    GMM(estimator = estimator, max_iterations = 50)\n)\n\nresult_chain = fit(chain, data, k)\nprintln(\"Chained algorithm objective: $(result_chain.objective)\")","category":"page"},{"location":"getting_started/#Nested-Metaheuristics","page":"Getting Started","title":"Nested Metaheuristics","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use one algorithm to improve another:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Use genetic algorithm with k-means++ as local search\nsophisticated = GeneticAlgorithm(\n    local_search = KmeansPlusPlus(max_iterations = 20),\n    max_iterations = 50\n)\n\nresult_sophisticated = fit(sophisticated, data, k)","category":"page"},{"location":"getting_started/#Working-with-Real-Data","page":"Getting Started","title":"Working with Real Data","text":"","category":"section"},{"location":"getting_started/#Loading-and-Preprocessing","page":"Getting Started","title":"Loading and Preprocessing","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using DelimitedFiles\n\n# Example: Load data from CSV (replace with your data)\n# data = readdlm(\"your_data.csv\", ',', Float64)\n\n# For this example, we'll create realistic data\nRandom.seed!(123)\ndata = vcat(\n    randn(40, 3) .+ [1, 1, 1],    # Cluster 1: high values\n    randn(40, 3) .+ [-1, -1, -1], # Cluster 2: low values\n    randn(40, 3) .+ [1, -1, 0]    # Cluster 3: mixed\n)\n\n# Standardize features (recommended for k-means)\nusing Statistics\ndata_std = (data .- mean(data, dims=1)) ./ std(data, dims=1)\n\nprintln(\"Original data range: $(minimum(data)) to $(maximum(data))\")\nprintln(\"Standardized data range: $(minimum(data_std)) to $(maximum(data_std))\")","category":"page"},{"location":"getting_started/#Choosing-the-Number-of-Clusters","page":"Getting Started","title":"Choosing the Number of Clusters","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Try different numbers of clusters\nk_values = 2:6\nobjectives = Float64[]\n\nfor k in k_values\n    result = fit(Kmeans(), data_std, k)\n    push!(objectives, result.objective)\nend\n\nprintln(\"Clustering objectives for different k:\")\nfor (i, k) in enumerate(k_values)\n    println(\"k=$k: objective=$(round(objectives[i], digits=3))\")\nend\n\n# Choose k with steepest decrease (elbow method)\nbest_k = k_values[argmin(objectives)]\nprintln(\"Suggested k: $best_k\")","category":"page"},{"location":"getting_started/#Performance-Tips","page":"Getting Started","title":"Performance Tips","text":"","category":"section"},{"location":"getting_started/#1.-Algorithm-Selection","page":"Getting Started","title":"1. Algorithm Selection","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# For speed: K-means\nfast_result = fit(Kmeans(max_iterations = 50), data, k)\n\n# For quality: K-means++\nquality_result = fit(KmeansPlusPlus(), data, k)\n\n# For robustness: Genetic Algorithm\nrobust_result = fit(GeneticAlgorithm(local_search = Kmeans()), data, k)","category":"page"},{"location":"getting_started/#2.-Parameter-Tuning","page":"Getting Started","title":"2. Parameter Tuning","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Fine-tune convergence\nprecise_kmeans = Kmeans(\n    tolerance = 1e-8,        # Stricter convergence\n    max_iterations = 1000    # More iterations allowed\n)\n\n# Quick and dirty clustering\nfast_kmeans = Kmeans(\n    tolerance = 1e-2,        # Looser convergence\n    max_iterations = 50      # Fewer iterations\n)","category":"page"},{"location":"getting_started/#3.-Reproducibility","page":"Getting Started","title":"3. Reproducibility","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Random, StableRNGs\n\n# Use StableRNG for cross-platform reproducibility\nrng = StableRNG(42)\nkmeans_reproducible = Kmeans(rng = rng)\nresult = fit(kmeans_reproducible, data, k)","category":"page"},{"location":"getting_started/#Common-Patterns","page":"Getting Started","title":"Common Patterns","text":"","category":"section"},{"location":"getting_started/#Batch-Processing","page":"Getting Started","title":"Batch Processing","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Process multiple datasets\ndatasets = [rand(100, 2), rand(150, 3), rand(200, 4)]\nk_values = [3, 4, 5]\n\nresults = []\nfor (data, k) in zip(datasets, k_values)\n    algorithm = KmeansPlusPlus(max_iterations = 100)\n    result = fit(algorithm, data, k)\n    push!(results, result)\n    println(\"Dataset $(length(results)): objective = $(result.objective)\")\nend","category":"page"},{"location":"getting_started/#Error-Handling","page":"Getting Started","title":"Error Handling","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"function safe_clustering(data, k, algorithm)\n    try\n        if size(data, 1) < k\n            error(\"Number of data points ($(size(data, 1))) must be ≥ k ($k)\")\n        end\n\n        result = fit(algorithm, data, k)\n\n        if !result.converged\n            @warn \"Algorithm did not converge\"\n        end\n\n        return result\n    catch e\n        @error \"Clustering failed: $e\"\n        return nothing\n    end\nend\n\n# Usage\nresult = safe_clustering(data, 3, Kmeans())\nif result !== nothing\n    println(\"Clustering successful: $(result.objective)\")\nend","category":"page"},{"location":"getting_started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Explore specific algorithms in the Local Search and Metaheuristic sections\nLearn about algorithm composition for advanced workflows\nCheck the API documentation for detailed parameter descriptions\nSee the research paper for theoretical background on the optimization techniques","category":"page"},{"location":"local_search/kmedoids/#K-medoids-Clustering","page":"k-medoids","title":"K-medoids Clustering","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"K-medoids is a robust clustering algorithm that uses actual data points (medoids) as cluster centers instead of computed centroids. This makes it more resistant to outliers and suitable for non-Euclidean distance metrics.","category":"page"},{"location":"local_search/kmedoids/#Algorithm-Overview","page":"k-medoids","title":"Algorithm Overview","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"K-medoids iteratively:","category":"page"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"Initialization: Select k data points as initial medoids\nAssignment: Assign each point to the nearest medoid\nUpdate: Find the optimal medoid for each cluster (point that minimizes total distance)\nRepeat: Continue until no improvement in objective","category":"page"},{"location":"local_search/kmedoids/#Basic-Usage","page":"k-medoids","title":"Basic Usage","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"using UnsupervisedClustering\n\n# Generate sample data\ndata = rand(100, 2)\nk = 3\n\n# Create and run k-medoids\nkmedoids = Kmedoids()\nresult = fit(kmedoids, data, k)\n\nprintln(\"Objective: $(result.objective)\")\nprintln(\"Medoids: $(result.centroids)\")","category":"page"},{"location":"local_search/kmedoids/#Comparison-with-K-means","page":"k-medoids","title":"Comparison with K-means","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"using UnsupervisedClustering, Random\n\n# Create data with outliers\nRandom.seed!(42)\nnormal_data = randn(90, 2)\noutliers = rand(10, 2) .* 10  # Add extreme outliers\ndata = vcat(normal_data, outliers)\n\n# Compare robustness\nalgorithms = [\n    (\"K-means\", Kmeans()),\n    (\"K-medoids\", Kmedoids())\n]\n\nprintln(\"Robustness Comparison:\")\nfor (name, alg) in algorithms\n    result = fit(alg, data, 3)\n    println(\"$name: objective = $(round(result.objective, digits=3))\")\nend","category":"page"},{"location":"local_search/kmedoids/#Configuration-Options","page":"k-medoids","title":"Configuration Options","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"# Customize k-medoids parameters\nkmedoids = Kmedoids(\n    metric = Euclidean(),        # Distance metric\n    max_iterations = 1000,       # Maximum iterations\n    verbose = false              # Print progress\n)\n\nresult = fit(kmedoids, data, k)","category":"page"},{"location":"local_search/kmedoids/#Working-with-Different-Distance-Metrics","page":"k-medoids","title":"Working with Different Distance Metrics","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"K-medoids works well with various distance metrics:","category":"page"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"using Distances\n\n# Try different distance metrics\nmetrics = [\n    (\"Euclidean\", Euclidean()),\n    (\"Manhattan\", Cityblock()),\n    (\"Chebyshev\", Chebyshev())\n]\n\nprintln(\"Distance Metric Comparison:\")\nfor (name, metric) in metrics\n    kmedoids = Kmedoids(metric = metric)\n    result = fit(kmedoids, data, 3)\n    println(\"$name: objective = $(round(result.objective, digits=3))\")\nend","category":"page"},{"location":"local_search/kmedoids/#Categorical-Data-Example","page":"k-medoids","title":"Categorical Data Example","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"K-medoids excels with categorical or mixed data:","category":"page"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"# Simulate categorical data (using Hamming distance)\nRandom.seed!(42)\ncategorical_data = rand(0:3, 100, 5)  # 5 categorical features with 4 levels each\n\n# Use Hamming distance for categorical data\nhamming_kmedoids = Kmedoids(metric = Hamming())\nresult = fit(hamming_kmedoids, categorical_data, 3)\n\nprintln(\"Categorical clustering objective: $(result.objective)\")","category":"page"},{"location":"local_search/kmedoids/#Advanced-Example-with-Preprocessing","page":"k-medoids","title":"Advanced Example with Preprocessing","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"using UnsupervisedClustering, Random, Statistics\n\n# Create complex dataset\nRandom.seed!(42)\ncluster1 = randn(40, 3) .+ [2, 2, 2]\ncluster2 = randn(40, 3) .+ [-2, -2, -2]\ncluster3 = randn(40, 3) .+ [2, -2, 0]\ndata = vcat(cluster1, cluster2, cluster3)\n\n# Add some outliers\noutliers = randn(20, 3) .* 5\ndata_with_outliers = vcat(data, outliers)\n\n# Configure robust k-medoids\nkmedoids = Kmedoids(\n    metric = Euclidean(),\n    max_iterations = 500,\n    verbose = true\n)\n\nresult = fit(kmedoids, data_with_outliers, 3)\n\nprintln(\"Robust clustering completed:\")\nprintln(\"  Objective: $(result.objective)\")\nprintln(\"  Iterations: $(result.iterations)\")\nprintln(\"  Medoids are actual data points: $(all(in.(eachrow(result.centroids), [eachrow(data_with_outliers)])))\")","category":"page"},{"location":"local_search/kmedoids/#Integration-with-Metaheuristics","page":"k-medoids","title":"Integration with Metaheuristics","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"# Use k-medoids with genetic algorithm\ngenetic_kmedoids = GeneticAlgorithm(\n    local_search = Kmedoids(max_iterations = 50),\n    max_iterations = 100\n)\n\n# Multi-start k-medoids\nmulti_kmedoids = MultiStart(\n    local_search = Kmedoids(),\n    max_iterations = 15\n)\n\n# Compare approaches\nstandard_result = fit(Kmedoids(), data, 3)\ngenetic_result = fit(genetic_kmedoids, data, 3)\nmulti_result = fit(multi_kmedoids, data, 3)\n\nprintln(\"Standard k-medoids: $(standard_result.objective)\")\nprintln(\"Genetic k-medoids: $(genetic_result.objective)\")\nprintln(\"Multi-start k-medoids: $(multi_result.objective)\")","category":"page"},{"location":"local_search/kmedoids/#Performance-Considerations","page":"k-medoids","title":"Performance Considerations","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"# For large datasets, limit iterations\nfast_kmedoids = Kmedoids(max_iterations = 50)\n\n# Compare timing\nlarge_data = rand(1000, 10)\n\n# Time standard vs fast versions\n@time result1 = fit(Kmedoids(), large_data, 5)\n@time result2 = fit(fast_kmedoids, large_data, 5)\n\nprintln(\"Standard: $(result1.objective) in $(result1.elapsed)s\")\nprintln(\"Fast: $(result2.objective) in $(result2.elapsed)s\")","category":"page"},{"location":"local_search/kmedoids/#When-to-Use-K-medoids","page":"k-medoids","title":"When to Use K-medoids","text":"","category":"section"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"K-medoids is preferred when:","category":"page"},{"location":"local_search/kmedoids/","page":"k-medoids","title":"k-medoids","text":"Outliers present: More robust than k-means to extreme values\nNon-Euclidean metrics: Works with any distance metric\nCategorical data: Natural choice for discrete features\nInterpretable centers: Medoids are actual data points\nSmall-medium datasets: Computationally more expensive than k-means","category":"page"},{"location":"local_search/kmedoids/#API-Reference","page":"k-medoids","title":"API Reference","text":"","category":"section"},{"location":"local_search/kmedoids/#UnsupervisedClustering.BalancedKmedoids","page":"k-medoids","title":"UnsupervisedClustering.BalancedKmedoids","text":"BalancedKmedoids(\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Float64 = DEFAULT_TOLERANCE\n    max_iterations::Int = DEFAULT_MAX_ITERATIONS\n)\n\nThe balanced k-medoids is a variation of the k-medoids algorithm that balances the number of data points assigned to each cluster. It uses the same parameters as the k-medoids algorithm.\n\nFields\n\nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the centroid positions between consecutive iterations.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\n\n\n\n\n\n","category":"type"},{"location":"local_search/kmedoids/#UnsupervisedClustering.Kmedoids","page":"k-medoids","title":"UnsupervisedClustering.Kmedoids","text":"Kmedoids(\n    verbose::Bool = DEFAULT_VERBOSE\n    rng::AbstractRNG = Random.GLOBAL_RNG\n    tolerance::Float64 = DEFAULT_TOLERANCE\n    max_iterations::Int = DEFAULT_MAX_ITERATIONS\n)\n\nThe k-medoids is a variation of k-means clustering algorithm that uses actual data points (medoids) as representatives of each cluster instead of the mean.\n\nFields\n\nverbose: controls whether the algorithm should display additional information during execution.\nrng: represents the random number generator to be used by the algorithm.\ntolerance: represents the convergence criterion for the algorithm. It determines the maximum change allowed in the centroid positions between consecutive iterations.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping, even if convergence has not been reached.\n\nReferences\n\n\n\n\n\n","category":"type"},{"location":"local_search/kmedoids/#UnsupervisedClustering.KmedoidsResult","page":"k-medoids","title":"UnsupervisedClustering.KmedoidsResult","text":"KmedoidsResult(\n    assignments::AbstractVector{<:Integer}\n    clusters::AbstractVector{<:Integer}\n    objective::Real\n    objective_per_cluster::AbstractVector{<:Real}\n    iterations::Integer\n    elapsed::Real\n    converged::Bool\n    k::Integer\n)\n\nKmedoidsResult struct represents the result of the k-medoids clustering algorithm.\n\nFields\n\nassignments: an integer vector that stores the cluster assignment for each data point.\nclusters: an integer vector representing each cluster's centroid.\nobjective: a floating-point number representing the objective function after running the algorithm. The objective function measures the quality of the clustering solution.\nobjective_per_cluster: a floating-point vector that stores the objective function of each cluster\niterations: an integer value indicating the number of iterations performed until the algorithm has converged or reached the maximum number of iterations\nelapsed: a floating-point number representing the time in seconds for the algorithm to complete.\nconverged: indicates whether the algorithm has converged to a solution.\nk: the number of clusters.\n\n\n\n\n\n","category":"type"},{"location":"local_search/kmedoids/#UnsupervisedClustering.fit!-Tuple{UnsupervisedClustering.AbstractKmedoids, AbstractMatrix{<:Real}, UnsupervisedClustering.KmedoidsResult}","page":"k-medoids","title":"UnsupervisedClustering.fit!","text":"fit!(\n    kmedoids::AbstractKmedoids,\n    distances::AbstractMatrix{<:Real},\n    result::KmedoidsResult\n)\n\nThe fit! function performs the k-medoids clustering algorithm on the given result as the initial point and updates the provided object with the clustering result.\n\nParameters:\n\nkmedoids: an instance representing the clustering settings and parameters.\ndistances: a floating-point matrix representing the pairwise distances between the data points.\nresult: a result object that will be updated with the clustering result.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\ndistances = pairwise(SqEuclidean(), data, dims = 1)\n\nkmedoids = Kmedoids()\nresult = KmedoidsResult(n, [1.0 2.0; 1.0 2.0])\nfit!(kmedoids, distances, result)\n\n\n\n\n\n","category":"method"},{"location":"local_search/kmedoids/#UnsupervisedClustering.fit-Tuple{UnsupervisedClustering.AbstractKmedoids, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"k-medoids","title":"UnsupervisedClustering.fit","text":"fit(\n    kmedoids::AbstractKmedoids,\n    distances::AbstractMatrix{<:Real},\n    initial_clusters::AbstractVector{<:Integer}\n)\n\nThe fit function performs the k-medoids clustering algorithm on the given data points as the initial point and returns a result object representing the clustering result.\n\nParameters:\n\nkmedoids: an instance representing the clustering settings and parameters.\ndistances: a floating-point matrix representing the pairwise distances between the data points.\ninitial_clusters: an integer vector where each element is the initial data point for each cluster.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\ndistances = pairwise(SqEuclidean(), data, dims = 1)\n\nkmedoids = Kmedoids()\nresult = fit(kmedoids, distances, [4, 12])\n\n\n\n\n\n","category":"method"},{"location":"local_search/kmedoids/#UnsupervisedClustering.fit-Tuple{UnsupervisedClustering.AbstractKmedoids, AbstractMatrix{<:Real}, Integer}","page":"k-medoids","title":"UnsupervisedClustering.fit","text":"fit(\n    kmedoids::AbstractKmedoids,\n    distances::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function performs the k-medoids clustering algorithm and returns a result object representing the clustering result.\n\nParameters:\n\nkmedoids: an instance representing the clustering settings and parameters.\ndistances: a floating-point matrix representing the pairwise distances between the data points.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\ndistances = pairwise(SqEuclidean(), data, dims = 1)\n\nkmedoids = Kmedoids()\nresult = fit(kmedoids, distances, k)\n\n\n\n\n\n","category":"method"},{"location":"ensemble/chain/#Clustering-Chain","page":"Clustering Chain","title":"Clustering Chain","text":"","category":"section"},{"location":"ensemble/chain/#UnsupervisedClustering.ClusteringChain","page":"Clustering Chain","title":"UnsupervisedClustering.ClusteringChain","text":"ClusteringChain{T}(algorithms::Vector{T}) where {T <: AbstractAlgorithm}\nClusteringChain(algorithms::AbstractAlgorithm...)\n\nClusteringChain represents a chain of clustering algorithms that are executed sequentially. It allows for applying multiple clustering algorithms in a specific order to refine and improve the clustering results.\n\nType Parameters\n\nT: algorithm type (concrete for same types, AbstractAlgorithm for mixed types)\n\nFields\n\nalgorithms: the vector of clustering algorithms that will be executed in sequence.\n\n\n\n\n\n","category":"type"},{"location":"ensemble/chain/#UnsupervisedClustering.fit-Union{Tuple{T}, Tuple{ClusteringChain{T}, AbstractMatrix{<:Real}, Integer}} where T<:UnsupervisedClustering.AbstractAlgorithm","page":"Clustering Chain","title":"UnsupervisedClustering.fit","text":"fit(chain::ClusteringChain, data::AbstractMatrix{<:Real}, k::Integer)\n\nThe fit function applies a sequence of clustering algorithms and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\n\nchain = ClusteringChain(kmeans, gmm)\nresult = fit(chain, data, k)\n\n\n\n\n\n","category":"method"},{"location":"metaheuristic/multi_start/#Multi-Start","page":"Multi-Start","title":"Multi-Start","text":"","category":"section"},{"location":"metaheuristic/multi_start/#UnsupervisedClustering.MultiStart","page":"Multi-Start","title":"UnsupervisedClustering.MultiStart","text":"MultiStart{LS}(\n    local_search::LS\n    verbose::Bool = DEFAULT_VERBOSE\n    max_iterations::Int = 200\n) where {LS <: AbstractAlgorithm}\n\nThe MultiStart approach repeatedly applies a clustering algorithm to generate multiple solutions with different initial points and selects the best solution.\n\nType Parameters\n\nLS: the specific type of the local search algorithm\n\nFields\n\nlocal_search: the clustering algorithm applied to improve the solution in each meta-heuristics iteration.\nverbose: controls whether the algorithm should display additional information during execution.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping.\n\n\n\n\n\n","category":"type"},{"location":"metaheuristic/multi_start/#UnsupervisedClustering.fit-Union{Tuple{LS}, Tuple{MultiStart{LS}, AbstractMatrix{<:Real}, Integer}} where LS<:UnsupervisedClustering.AbstractAlgorithm","page":"Multi-Start","title":"UnsupervisedClustering.fit","text":"fit(\n    meta::MultiStart,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function applies a multi-start to a clustering problem and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\nmulti_start = MultiStart(local_search = kmeans)\nresult = fit(multi_start, data, k)\n\n\n\n\n\n","category":"method"},{"location":"#UnsupervisedClustering.jl","page":"Home","title":"UnsupervisedClustering.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package providing a unified interface for unsupervised clustering algorithms with advanced optimization techniques to escape local optima and reduce overfitting.","category":"page"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Unified Interface: All clustering algorithms use the same fit(algorithm, data, k) pattern\nAdvanced Optimization: Metaheuristic approaches including genetic algorithms and multi-start strategies\nType-Stable: Modern Julia design with parameterized types for zero runtime overhead\nComposable: Mix and match algorithms through chaining and metaheuristic composition","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using UnsupervisedClustering\nusing RegularizedCovarianceMatrices\n\n# Generate sample data\nn = 100\nd = 2\nk = 3\ndata = rand(n, d)\n\n# Local search algorithms\nkmeans = Kmeans()\ngmm = GMM(estimator = EmpiricalCovarianceMatrix(n, d))\n\n# Metaheuristic algorithms\ngenetic = GeneticAlgorithm(local_search = kmeans)\nmulti_start = MultiStart(local_search = kmeans)\nrandom_swap = RandomSwap(local_search = kmeans)\n\n# All use the same fit function!\nresult1 = fit(kmeans, data, k)\nresult2 = fit(genetic, data, k)\nresult3 = fit(multi_start, data, k)\n\n# The unified interface enables some compositions:\nchain = ClusteringChain(kmeans, gmm)\nresult4 = fit(chain, data, k)","category":"page"},{"location":"#Algorithm-Categories","page":"Home","title":"Algorithm Categories","text":"","category":"section"},{"location":"#Local-Search-Algorithms","page":"Home","title":"Local Search Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"K-means: Classic centroid-based clustering with Lloyd's algorithm\nK-means++: Improved initialization for better clustering quality\nK-medoids: Robust clustering using actual data points as centers\nGMM: Gaussian Mixture Models with EM algorithm","category":"page"},{"location":"#Metaheuristic-Algorithms","page":"Home","title":"Metaheuristic Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Genetic Algorithm: Evolutionary approach for global optimization\nMulti-Start: Multiple random initializations with best result selection\nRandom Swap: Perturbation-based local search escape","category":"page"},{"location":"#Ensemble-Methods","page":"Home","title":"Ensemble Methods","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Clustering Chain: Sequential algorithm composition for refined results","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"UnsupervisedClustering\")","category":"page"},{"location":"#Related-Packages","page":"Home","title":"Related Packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package integrates with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"RegularizedCovarianceMatrices.jl: For advanced GMM regularization techniques\nDistances.jl: For distance metrics in clustering algorithms","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you find UnsupervisedClustering useful in your work, we kindly request that you cite the following paper:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{sampaio2024regularization,\n  title={Regularization and optimization in model-based clustering},\n  author={Sampaio, Raphael Araujo and Garcia, Joaquim Dias and Poggi, Marcus and Vidal, Thibaut},\n  journal={Pattern Recognition},\n  pages={110310},\n  year={2024},\n  publisher={Elsevier}\n}","category":"page"},{"location":"metaheuristic/genetic_algorithm/#Genetic-Algorithm","page":"Genetic Algorithm","title":"Genetic Algorithm","text":"","category":"section"},{"location":"metaheuristic/genetic_algorithm/#UnsupervisedClustering.GeneticAlgorithm","page":"Genetic Algorithm","title":"UnsupervisedClustering.GeneticAlgorithm","text":"GeneticAlgorithm{LS}(\n    local_search::LS\n    verbose::Bool = DEFAULT_VERBOSE\n    max_iterations::Int = 200\n    max_iterations_without_improvement::Int = 150\n    π_min::Int = 40\n    π_max::Int = 50\n) where {LS <: AbstractAlgorithm}\n\nGeneticAlgorithm represents a clustering algorithm that utilizes a genetic algorithm approach to optimize cluster assignments. It combines evolutionary computation and local search elements to find high-quality clustering solutions.\n\nType Parameters\n\nLS: the specific type of the local search algorithm (e.g., Kmeans, Kmedoids)\n\nFields\n\nlocal_search: the clustering algorithm applied to improve the solution in each meta-heuristics iteration.\nverbose: controls whether the algorithm should display additional information during execution.\nmax_iterations: represents the maximum number of iterations the algorithm will perform before stopping.\nmax_iterations_without_improvement: represents the maximum number of iterations allowed without improving the best solution.\nπ_max: the maximum population size used in the genetic algorithm.\nπ_min: the minimum population size used in the genetic algorithm.\n\nReferences\n\n\n\n\n\n","category":"type"},{"location":"metaheuristic/genetic_algorithm/#UnsupervisedClustering.fit-Union{Tuple{LS}, Tuple{GeneticAlgorithm{LS}, AbstractMatrix{<:Real}, Integer}} where LS<:UnsupervisedClustering.AbstractAlgorithm","page":"Genetic Algorithm","title":"UnsupervisedClustering.fit","text":"fit(\n    meta::GeneticAlgorithm,\n    data::AbstractMatrix{<:Real},\n    k::Integer\n)\n\nThe fit function applies a genetic algorithm to a clustering problem and returns a result object representing the clustering outcome.\n\nParameters:\n\nmeta: an instance representing the clustering settings and parameters.\ndata: a floating-point matrix, where each row represents a data point, and each column represents a feature.\nk: an integer representing the number of clusters.\n\nExample\n\nn = 100\nd = 2\nk = 2\n\ndata = rand(n, d)\n\nkmeans = Kmeans()\ngenetic_algorithm = GeneticAlgorithm(local_search = kmeans)\nresult = fit(genetic_algorithm, data, k)\n\n\n\n\n\n","category":"method"}]
}
